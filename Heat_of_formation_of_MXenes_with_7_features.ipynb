{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score,mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n7-feature dataset is run on Simple Linear Regression, LASSO, Ridge, Random Forest and Neural Network models for the prediction of Heat of Formation of MXenes.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv(\"data_for_traing_ml_models.csv\")\n",
    "\n",
    "#####################\n",
    "\n",
    "'''\n",
    "7-feature dataset is run on Simple Linear Regression, LASSO, Ridge, Random Forest and Neural Network models for the prediction of Heat of Formation of MXenes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Work_function</th>\n",
       "      <th>Heat_of_formation</th>\n",
       "      <th>Energy_above_convex_hull</th>\n",
       "      <th>Length(x)</th>\n",
       "      <th>Radius_of_orbital_of_metal</th>\n",
       "      <th>Radius_of_orbital_of_support</th>\n",
       "      <th>Radius_of_orbital_of_termination</th>\n",
       "      <th>Ionisation_potential_of_metal</th>\n",
       "      <th>Ionisation_potential_of_support</th>\n",
       "      <th>Ionisation_potential_of_termination</th>\n",
       "      <th>Electronegativity_of_metal</th>\n",
       "      <th>Electronegativity_of_support</th>\n",
       "      <th>Electronegativity_of_termination</th>\n",
       "      <th>Electron_affinity_of_metal</th>\n",
       "      <th>Electron_affinity_of_support</th>\n",
       "      <th>Electron_affinity_of_termination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.264700</td>\n",
       "      <td>-1.062837</td>\n",
       "      <td>0.315639</td>\n",
       "      <td>311.577091</td>\n",
       "      <td>65.518545</td>\n",
       "      <td>54.219636</td>\n",
       "      <td>45.983636</td>\n",
       "      <td>6.980777</td>\n",
       "      <td>12.891248</td>\n",
       "      <td>12.719608</td>\n",
       "      <td>1.601527</td>\n",
       "      <td>2.794109</td>\n",
       "      <td>2.951018</td>\n",
       "      <td>0.394964</td>\n",
       "      <td>0.598422</td>\n",
       "      <td>1.755294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.530484</td>\n",
       "      <td>0.835387</td>\n",
       "      <td>0.228956</td>\n",
       "      <td>20.841980</td>\n",
       "      <td>16.428133</td>\n",
       "      <td>5.409809</td>\n",
       "      <td>13.987596</td>\n",
       "      <td>0.518915</td>\n",
       "      <td>1.639873</td>\n",
       "      <td>3.765861</td>\n",
       "      <td>0.333816</td>\n",
       "      <td>0.245445</td>\n",
       "      <td>0.903641</td>\n",
       "      <td>0.364981</td>\n",
       "      <td>0.667210</td>\n",
       "      <td>1.099077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.347678</td>\n",
       "      <td>-2.984132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>262.100000</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>48.800000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>6.217000</td>\n",
       "      <td>11.260300</td>\n",
       "      <td>6.217000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.792242</td>\n",
       "      <td>-1.646810</td>\n",
       "      <td>0.162571</td>\n",
       "      <td>294.700000</td>\n",
       "      <td>48.900000</td>\n",
       "      <td>48.800000</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>6.633900</td>\n",
       "      <td>11.260300</td>\n",
       "      <td>7.980000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.537063</td>\n",
       "      <td>-1.079087</td>\n",
       "      <td>0.268825</td>\n",
       "      <td>312.500000</td>\n",
       "      <td>74.600000</td>\n",
       "      <td>59.600000</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.260300</td>\n",
       "      <td>13.017000</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>1.262000</td>\n",
       "      <td>1.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.349280</td>\n",
       "      <td>-0.483631</td>\n",
       "      <td>0.424251</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>59.600000</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>7.264100</td>\n",
       "      <td>14.534100</td>\n",
       "      <td>13.618000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>1.262000</td>\n",
       "      <td>1.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.271891</td>\n",
       "      <td>0.957153</td>\n",
       "      <td>1.138551</td>\n",
       "      <td>378.300000</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>59.600000</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>7.980000</td>\n",
       "      <td>14.534100</td>\n",
       "      <td>17.422800</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>1.262000</td>\n",
       "      <td>3.401000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Work_function  Heat_of_formation  Energy_above_convex_hull   Length(x)  \\\n",
       "count     275.000000         275.000000                275.000000  275.000000   \n",
       "mean        4.264700          -1.062837                  0.315639  311.577091   \n",
       "std         1.530484           0.835387                  0.228956   20.841980   \n",
       "min         1.347678          -2.984132                  0.000000  262.100000   \n",
       "25%         2.792242          -1.646810                  0.162571  294.700000   \n",
       "50%         4.537063          -1.079087                  0.268825  312.500000   \n",
       "75%         5.349280          -0.483631                  0.424251  324.000000   \n",
       "max         7.271891           0.957153                  1.138551  378.300000   \n",
       "\n",
       "       Radius_of_orbital_of_metal  Radius_of_orbital_of_support  \\\n",
       "count                  275.000000                    275.000000   \n",
       "mean                    65.518545                     54.219636   \n",
       "std                     16.428133                      5.409809   \n",
       "min                     38.900000                     48.800000   \n",
       "25%                     48.900000                     48.800000   \n",
       "50%                     74.600000                     59.600000   \n",
       "75%                     79.000000                     59.600000   \n",
       "max                     86.500000                     59.600000   \n",
       "\n",
       "       Radius_of_orbital_of_termination  Ionisation_potential_of_metal  \\\n",
       "count                        275.000000                     275.000000   \n",
       "mean                          45.983636                       6.980777   \n",
       "std                           13.987596                       0.518915   \n",
       "min                           36.000000                       6.217000   \n",
       "25%                           38.900000                       6.633900   \n",
       "50%                           41.400000                       6.825000   \n",
       "75%                           41.400000                       7.264100   \n",
       "max                           86.500000                       7.980000   \n",
       "\n",
       "       Ionisation_potential_of_support  Ionisation_potential_of_termination  \\\n",
       "count                       275.000000                           275.000000   \n",
       "mean                         12.891248                            12.719608   \n",
       "std                           1.639873                             3.765861   \n",
       "min                          11.260300                             6.217000   \n",
       "25%                          11.260300                             7.980000   \n",
       "50%                          11.260300                            13.017000   \n",
       "75%                          14.534100                            13.618000   \n",
       "max                          14.534100                            17.422800   \n",
       "\n",
       "       Electronegativity_of_metal  Electronegativity_of_support  \\\n",
       "count                  275.000000                    275.000000   \n",
       "mean                     1.601527                      2.794109   \n",
       "std                      0.333816                      0.245445   \n",
       "min                      1.220000                      2.550000   \n",
       "25%                      1.330000                      2.550000   \n",
       "50%                      1.540000                      2.550000   \n",
       "75%                      1.630000                      3.040000   \n",
       "max                      2.360000                      3.040000   \n",
       "\n",
       "       Electronegativity_of_termination  Electron_affinity_of_metal  \\\n",
       "count                        275.000000                  275.000000   \n",
       "mean                           2.951018                    0.394964   \n",
       "std                            0.903641                    0.364981   \n",
       "min                            1.220000                   -0.500000   \n",
       "25%                            2.360000                    0.189000   \n",
       "50%                            2.820000                    0.323000   \n",
       "75%                            3.440000                    0.749000   \n",
       "max                            3.980000                    0.894000   \n",
       "\n",
       "       Electron_affinity_of_support  Electron_affinity_of_termination  \n",
       "count                    275.000000                        275.000000  \n",
       "mean                       0.598422                          1.755294  \n",
       "std                        0.667210                          1.099077  \n",
       "min                       -0.070000                         -0.500000  \n",
       "25%                       -0.070000                          0.894000  \n",
       "50%                        1.262000                          1.439000  \n",
       "75%                        1.262000                          1.827000  \n",
       "max                        1.262000                          3.401000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the dataset\n",
    "dataset.head()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Work_function</th>\n",
       "      <th>Heat_of_formation</th>\n",
       "      <th>Energy_above_convex_hull</th>\n",
       "      <th>Length(x)</th>\n",
       "      <th>Radius_of_orbital_of_metal</th>\n",
       "      <th>Radius_of_orbital_of_support</th>\n",
       "      <th>Radius_of_orbital_of_termination</th>\n",
       "      <th>Ionisation_potential_of_metal</th>\n",
       "      <th>Ionisation_potential_of_support</th>\n",
       "      <th>Ionisation_potential_of_termination</th>\n",
       "      <th>Electronegativity_of_metal</th>\n",
       "      <th>Electronegativity_of_support</th>\n",
       "      <th>Electronegativity_of_termination</th>\n",
       "      <th>Electron_affinity_of_metal</th>\n",
       "      <th>Electron_affinity_of_support</th>\n",
       "      <th>Electron_affinity_of_termination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.271378</td>\n",
       "      <td>-1.345952</td>\n",
       "      <td>0.242880</td>\n",
       "      <td>330.4</td>\n",
       "      <td>82.9</td>\n",
       "      <td>59.6</td>\n",
       "      <td>41.4</td>\n",
       "      <td>6.8250</td>\n",
       "      <td>11.2603</td>\n",
       "      <td>13.0170</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.178</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.588009</td>\n",
       "      <td>-1.199954</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>335.0</td>\n",
       "      <td>53.9</td>\n",
       "      <td>59.6</td>\n",
       "      <td>41.4</td>\n",
       "      <td>6.5614</td>\n",
       "      <td>11.2603</td>\n",
       "      <td>13.0170</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.209840</td>\n",
       "      <td>-0.317344</td>\n",
       "      <td>0.473693</td>\n",
       "      <td>290.6</td>\n",
       "      <td>74.6</td>\n",
       "      <td>59.6</td>\n",
       "      <td>41.4</td>\n",
       "      <td>7.9800</td>\n",
       "      <td>11.2603</td>\n",
       "      <td>13.0170</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.463148</td>\n",
       "      <td>-0.594739</td>\n",
       "      <td>0.327053</td>\n",
       "      <td>285.0</td>\n",
       "      <td>70.2</td>\n",
       "      <td>48.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>7.0942</td>\n",
       "      <td>14.5341</td>\n",
       "      <td>13.0170</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.329750</td>\n",
       "      <td>-1.597694</td>\n",
       "      <td>0.163033</td>\n",
       "      <td>298.9</td>\n",
       "      <td>48.9</td>\n",
       "      <td>48.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>6.8282</td>\n",
       "      <td>14.5341</td>\n",
       "      <td>13.0170</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.429486</td>\n",
       "      <td>-1.645322</td>\n",
       "      <td>0.315790</td>\n",
       "      <td>320.4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>48.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>6.6339</td>\n",
       "      <td>14.5341</td>\n",
       "      <td>13.0170</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.244289</td>\n",
       "      <td>-0.755420</td>\n",
       "      <td>0.290092</td>\n",
       "      <td>293.1</td>\n",
       "      <td>38.9</td>\n",
       "      <td>59.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.4340</td>\n",
       "      <td>11.2603</td>\n",
       "      <td>17.4228</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.98</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>1.262</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.424606</td>\n",
       "      <td>-1.265524</td>\n",
       "      <td>0.269073</td>\n",
       "      <td>316.7</td>\n",
       "      <td>78.4</td>\n",
       "      <td>59.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.8900</td>\n",
       "      <td>11.2603</td>\n",
       "      <td>17.4228</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1.262</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.468438</td>\n",
       "      <td>-1.715968</td>\n",
       "      <td>0.168985</td>\n",
       "      <td>348.9</td>\n",
       "      <td>86.5</td>\n",
       "      <td>59.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.2170</td>\n",
       "      <td>11.2603</td>\n",
       "      <td>17.4228</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.308</td>\n",
       "      <td>1.262</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Work_function  Heat_of_formation  Energy_above_convex_hull  Length(x)  \\\n",
       "0       2.271378          -1.345952                  0.242880      330.4   \n",
       "1       1.588009          -1.199954                  0.452744      335.0   \n",
       "2       2.209840          -0.317344                  0.473693      290.6   \n",
       "3       2.463148          -0.594739                  0.327053      285.0   \n",
       "4       2.329750          -1.597694                  0.163033      298.9   \n",
       "5       2.429486          -1.645322                  0.315790      320.4   \n",
       "6       6.244289          -0.755420                  0.290092      293.1   \n",
       "7       4.424606          -1.265524                  0.269073      316.7   \n",
       "8       4.468438          -1.715968                  0.168985      348.9   \n",
       "\n",
       "   Radius_of_orbital_of_metal  Radius_of_orbital_of_support  \\\n",
       "0                        82.9                          59.6   \n",
       "1                        53.9                          59.6   \n",
       "2                        74.6                          59.6   \n",
       "3                        70.2                          48.8   \n",
       "4                        48.9                          48.8   \n",
       "5                        79.0                          48.8   \n",
       "6                        38.9                          59.6   \n",
       "7                        78.4                          59.6   \n",
       "8                        86.5                          59.6   \n",
       "\n",
       "   Radius_of_orbital_of_termination  Ionisation_potential_of_metal  \\\n",
       "0                              41.4                         6.8250   \n",
       "1                              41.4                         6.5614   \n",
       "2                              41.4                         7.9800   \n",
       "3                              41.4                         7.0942   \n",
       "4                              41.4                         6.8282   \n",
       "5                              41.4                         6.6339   \n",
       "6                              36.0                         7.4340   \n",
       "7                              36.0                         7.8900   \n",
       "8                              36.0                         6.2170   \n",
       "\n",
       "   Ionisation_potential_of_support  Ionisation_potential_of_termination  \\\n",
       "0                          11.2603                              13.0170   \n",
       "1                          11.2603                              13.0170   \n",
       "2                          11.2603                              13.0170   \n",
       "3                          14.5341                              13.0170   \n",
       "4                          14.5341                              13.0170   \n",
       "5                          14.5341                              13.0170   \n",
       "6                          11.2603                              17.4228   \n",
       "7                          11.2603                              17.4228   \n",
       "8                          11.2603                              17.4228   \n",
       "\n",
       "   Electronegativity_of_metal  Electronegativity_of_support  \\\n",
       "0                        1.30                          2.55   \n",
       "1                        1.36                          2.55   \n",
       "2                        2.36                          2.55   \n",
       "3                        2.16                          3.04   \n",
       "4                        1.54                          3.04   \n",
       "5                        1.33                          3.04   \n",
       "6                        1.55                          2.55   \n",
       "7                        1.50                          2.55   \n",
       "8                        1.22                          2.55   \n",
       "\n",
       "   Electronegativity_of_termination  Electron_affinity_of_metal  \\\n",
       "0                              2.82                       0.178   \n",
       "1                              2.82                       0.189   \n",
       "2                              2.82                       0.816   \n",
       "3                              2.82                       0.749   \n",
       "4                              2.82                       0.087   \n",
       "5                              2.82                       0.427   \n",
       "6                              3.98                      -0.500   \n",
       "7                              3.98                       0.323   \n",
       "8                              3.98                       0.308   \n",
       "\n",
       "   Electron_affinity_of_support  Electron_affinity_of_termination  \n",
       "0                         1.262                             1.827  \n",
       "1                         1.262                             1.827  \n",
       "2                         1.262                             1.827  \n",
       "3                        -0.070                             1.827  \n",
       "4                        -0.070                             1.827  \n",
       "5                        -0.070                             1.827  \n",
       "6                         1.262                             3.401  \n",
       "7                         1.262                             3.401  \n",
       "8                         1.262                             3.401  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = dataset[['Work_function','Heat_of_formation','Energy_above_convex_hull','Length(x)','Radius_of_orbital_of_metal','Radius_of_orbital_of_support','Radius_of_orbital_of_termination','Ionisation_potential_of_metal','Ionisation_potential_of_support','Ionisation_potential_of_termination','Electronegativity_of_metal','Electronegativity_of_support','Electronegativity_of_termination','Electron_affinity_of_metal','Electron_affinity_of_support','Electron_affinity_of_termination']]\n",
    "cdf.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "\n",
      "Train set metrics:\n",
      "MAE: 0.30\n",
      "R2 Score: 0.79\n",
      "LinearRegression\n",
      "\n",
      "\n",
      "Test set metrics:\n",
      "MAE: 0.29\n",
      "R2 Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "###################### Simple Linear Regression\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Assuming 'HoF', 'E_Hull', 'Length(x)', ..., are your features\n",
    "features = ['Radius_of_orbital_of_metal','Radius_of_orbital_of_termination', 'Ionisation_potential_of_metal','Ionisation_potential_of_support','Electronegativity_of_metal','Electronegativity_of_termination','Electron_affinity_of_metal']\n",
    "target = ['Heat_of_formation']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train set\n",
    "train_x = np.asanyarray(train[features])\n",
    "train_y = np.asanyarray(train[target])\n",
    "\n",
    "# Test set\n",
    "test_x = np.asanyarray(test[features])\n",
    "test_y = np.asanyarray(test[target])\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "regr.fit(train_x, train_y)\n",
    "\n",
    "# Predictions on train and test sets\n",
    "train_y_hat = regr.predict(train_x)\n",
    "test_y_hat = regr.predict(test_x)\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the train set\n",
    "train_mse = mean_squared_error(train_y, train_y_hat)\n",
    "train_mae = mean_absolute_error(train_y, train_y_hat)\n",
    "train_r2 = r2_score(train_y, train_y_hat)\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the test set\n",
    "test_mse = mean_squared_error(test_y, test_y_hat)\n",
    "test_mae = mean_absolute_error(test_y, test_y_hat)\n",
    "test_r2 = r2_score(test_y, test_y_hat)\n",
    "\n",
    "# Print the results\n",
    "print('Linear Regression\\n')\n",
    "print(\"Train set metrics:\")\n",
    "#print(\"MSE: %.2f\" % train_mse)\n",
    "print(\"MAE: %.2f\" % train_mae)\n",
    "print(\"R2 Score: %.2f\" % train_r2)\n",
    "print('LinearRegression\\n')\n",
    "print(\"\\nTest set metrics:\")\n",
    "#print(\"MSE: %.2f\" % test_mse)\n",
    "print(\"MAE: %.2f\" % test_mae)\n",
    "print(\"R2 Score: %.2f\" % test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO Regression\n",
      "Train set metrics:\n",
      "MAE: 0.54\n",
      "R2 Score: 0.31\n",
      "\n",
      "Test set metrics:\n",
      "MAE: 0.58\n",
      "R2 Score: 0.37\n"
     ]
    }
   ],
   "source": [
    "####################### LASSO Regression\n",
    "\n",
    "\n",
    "# Initialize LASSO model\n",
    "regr1 = Lasso()\n",
    "# Create and train the LASSO Regression model\n",
    "regr1.fit(train_x, train_y)\n",
    "\n",
    "# Predictions on train and test sets\n",
    "train_y_hat1 = regr1.predict(train_x)\n",
    "test_y_hat1 = regr1.predict(test_x)\n",
    "\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the train set\n",
    "train_mse1 = mean_squared_error(train_y, train_y_hat1)\n",
    "train_mae1 = mean_absolute_error(train_y, train_y_hat1)\n",
    "train_r2_1 = r2_score(train_y, train_y_hat1)\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the test set\n",
    "test_mse1 = mean_squared_error(test_y, test_y_hat1)\n",
    "test_mae1 = mean_absolute_error(test_y, test_y_hat1)\n",
    "test_r2_1 = r2_score(test_y, test_y_hat1)\n",
    "\n",
    "# Print the results\n",
    "print('LASSO Regression')\n",
    "print(\"Train set metrics:\")\n",
    "#print(\"MSE: %.2f\" % train_mse1)\n",
    "print(\"MAE: %.2f\" % train_mae1)\n",
    "print(\"R2 Score: %.2f\" % train_r2_1)\n",
    "\n",
    "print(\"\\nTest set metrics:\")\n",
    "#print(\"MSE: %.2f\" % test_mse1)\n",
    "print(\"MAE: %.2f\" % test_mae1)\n",
    "print(\"R2 Score: %.2f\" % test_r2_1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression\n",
      "\n",
      "Train set metrics:\n",
      "MAE: 0.30\n",
      "R2 Score: 0.79\n",
      "\n",
      "Test set metrics:\n",
      "MAE: 0.29\n",
      "R2 Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "####################### Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Ridge Regression\n",
    "regr2 = Ridge()\n",
    "# Create and train the Ridge Regression model\n",
    "regr2.fit(train_x, train_y)\n",
    "\n",
    "# Predictions on train and test sets\n",
    "train_y_hat2 = regr2.predict(train_x)\n",
    "test_y_hat2 = regr2.predict(test_x)\n",
    "\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the train set\n",
    "train_mse2 = mean_squared_error(train_y, train_y_hat2)\n",
    "train_mae2 = mean_absolute_error(train_y, train_y_hat2)\n",
    "train_r2_2 = r2_score(train_y, train_y_hat2)\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the test set\n",
    "test_mse2 = mean_squared_error(test_y, test_y_hat2)\n",
    "test_mae2 = mean_absolute_error(test_y, test_y_hat2)\n",
    "test_r2_2 = r2_score(test_y, test_y_hat2)\n",
    "\n",
    "# Print the results\n",
    "print('Ridge Regression\\n')\n",
    "print(\"Train set metrics:\")\n",
    "#print(\"MSE: %.2f\" % train_mse2)\n",
    "print(\"MAE: %.2f\" % train_mae2)\n",
    "print(\"R2 Score: %.2f\" % train_r2_2)\n",
    "\n",
    "print(\"\\nTest set metrics:\")\n",
    "#print(\"MSE: %.2f\" % test_mse2)\n",
    "print(\"MAE: %.2f\" % test_mae2)\n",
    "print(\"R2 Score: %.2f\" % test_r2_2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression \n",
      "\n",
      "Train set metrics:\n",
      "MAE: 0.15\n",
      "R2 Score: 0.94\n",
      "\n",
      "Test set metrics:\n",
      "MAE: 0.23\n",
      "R2 Score: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azaan/anaconda3/envs/data_science/lib/python3.9/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "###################### Random Forest Regression\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Random Forest Regression\n",
    "regr3 = RandomForestRegressor()\n",
    "# Create and train the Random Forest Regression model\n",
    "regr3.fit(train_x, train_y)\n",
    "\n",
    "# Predictions on train and test sets\n",
    "train_y_hat3 = regr3.predict(train_x)\n",
    "test_y_hat3 = regr3.predict(test_x)\n",
    "\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the train set\n",
    "train_mse3 = mean_squared_error(train_y, train_y_hat3)\n",
    "train_mae3 = mean_absolute_error(train_y, train_y_hat3)\n",
    "train_r2_3 = r2_score(train_y, train_y_hat3)\n",
    "\n",
    "# Calculate MSE, MAE, and R2 for the test set\n",
    "test_mse3 = mean_squared_error(test_y, test_y_hat3)\n",
    "test_mae3 = mean_absolute_error(test_y, test_y_hat3)\n",
    "test_r2_3 = r2_score(test_y, test_y_hat3)\n",
    "\n",
    "# Print the results\n",
    "print('Random Forest Regression \\n')\n",
    "print(\"Train set metrics:\")\n",
    "#print(\"MSE: %.2f\" % train_mse3)\n",
    "print(\"MAE: %.2f\" % train_mae3)\n",
    "print(\"R2 Score: %.2f\" % train_r2_3)\n",
    "\n",
    "print(\"\\nTest set metrics:\")\n",
    "#print(\"MSE: %.2f\" % test_mse3)\n",
    "print(\"MAE: %.2f\" % test_mae3)\n",
    "print(\"R2 Score: %.2f\" % test_r2_3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################## Neural Network Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azaan/anaconda3/envs/data_science/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 7.2288 - val_loss: 7.1887\n",
      "Epoch 2/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.8997 - val_loss: 7.0474\n",
      "Epoch 3/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.7287 - val_loss: 6.9102\n",
      "Epoch 4/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.5395 - val_loss: 6.7931\n",
      "Epoch 5/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.4719 - val_loss: 6.6932\n",
      "Epoch 6/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.3982 - val_loss: 6.5962\n",
      "Epoch 7/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.2199 - val_loss: 6.5040\n",
      "Epoch 8/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.2455 - val_loss: 6.3866\n",
      "Epoch 9/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.1242 - val_loss: 6.2691\n",
      "Epoch 10/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.0262 - val_loss: 6.1629\n",
      "Epoch 11/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.9161 - val_loss: 6.0784\n",
      "Epoch 12/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.8915 - val_loss: 6.0037\n",
      "Epoch 13/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.8568 - val_loss: 5.9473\n",
      "Epoch 14/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.7972 - val_loss: 5.8613\n",
      "Epoch 15/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.6807 - val_loss: 5.7744\n",
      "Epoch 16/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.5761 - val_loss: 5.6958\n",
      "Epoch 17/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.5183 - val_loss: 5.6139\n",
      "Epoch 18/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.4681 - val_loss: 5.5437\n",
      "Epoch 19/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.4322 - val_loss: 5.4915\n",
      "Epoch 20/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.4045 - val_loss: 5.4454\n",
      "Epoch 21/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.3591 - val_loss: 5.3824\n",
      "Epoch 22/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.2842 - val_loss: 5.3175\n",
      "Epoch 23/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.2045 - val_loss: 5.2508\n",
      "Epoch 24/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.1625 - val_loss: 5.1945\n",
      "Epoch 25/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.1047 - val_loss: 5.1418\n",
      "Epoch 26/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.1169 - val_loss: 5.0876\n",
      "Epoch 27/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.9891 - val_loss: 5.0416\n",
      "Epoch 28/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9322 - val_loss: 4.9912\n",
      "Epoch 29/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9136 - val_loss: 4.9460\n",
      "Epoch 30/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9017 - val_loss: 4.8859\n",
      "Epoch 31/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.8113 - val_loss: 4.8282\n",
      "Epoch 32/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7440 - val_loss: 4.7823\n",
      "Epoch 33/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6960 - val_loss: 4.7340\n",
      "Epoch 34/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.6984 - val_loss: 4.6876\n",
      "Epoch 35/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.5747 - val_loss: 4.6355\n",
      "Epoch 36/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.5854 - val_loss: 4.5823\n",
      "Epoch 37/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.4737 - val_loss: 4.5379\n",
      "Epoch 38/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5086 - val_loss: 4.4788\n",
      "Epoch 39/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4178 - val_loss: 4.4190\n",
      "Epoch 40/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4352 - val_loss: 4.3753\n",
      "Epoch 41/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.3424 - val_loss: 4.3487\n",
      "Epoch 42/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3107 - val_loss: 4.3046\n",
      "Epoch 43/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2404 - val_loss: 4.2461\n",
      "Epoch 44/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.2032 - val_loss: 4.1978\n",
      "Epoch 45/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.1644 - val_loss: 4.1500\n",
      "Epoch 46/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1113 - val_loss: 4.1143\n",
      "Epoch 47/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0943 - val_loss: 4.0801\n",
      "Epoch 48/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0384 - val_loss: 4.0316\n",
      "Epoch 49/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.9651 - val_loss: 3.9886\n",
      "Epoch 50/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.9616 - val_loss: 3.9488\n",
      "Epoch 51/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.8776 - val_loss: 3.8994\n",
      "Epoch 52/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8699 - val_loss: 3.8454\n",
      "Epoch 53/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8223 - val_loss: 3.8037\n",
      "Epoch 54/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8078 - val_loss: 3.7582\n",
      "Epoch 55/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.7665 - val_loss: 3.7228\n",
      "Epoch 56/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.7027 - val_loss: 3.6704\n",
      "Epoch 57/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.6780 - val_loss: 3.6211\n",
      "Epoch 58/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.6518 - val_loss: 3.5752\n",
      "Epoch 59/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.6097 - val_loss: 3.5497\n",
      "Epoch 60/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.5915 - val_loss: 3.5065\n",
      "Epoch 61/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5257 - val_loss: 3.4745\n",
      "Epoch 62/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.4695 - val_loss: 3.4466\n",
      "Epoch 63/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.4736 - val_loss: 3.4175\n",
      "Epoch 64/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.4288 - val_loss: 3.3689\n",
      "Epoch 65/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.3687 - val_loss: 3.3372\n",
      "Epoch 66/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.3357 - val_loss: 3.3079\n",
      "Epoch 67/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.2697 - val_loss: 3.2613\n",
      "Epoch 68/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.2600 - val_loss: 3.2267\n",
      "Epoch 69/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.2073 - val_loss: 3.2078\n",
      "Epoch 70/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.2259 - val_loss: 3.1718\n",
      "Epoch 71/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.2052 - val_loss: 3.1341\n",
      "Epoch 72/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.1734 - val_loss: 3.1050\n",
      "Epoch 73/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.1319 - val_loss: 3.0777\n",
      "Epoch 74/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.0172 - val_loss: 3.0317\n",
      "Epoch 75/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.0694 - val_loss: 2.9897\n",
      "Epoch 76/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.0070 - val_loss: 2.9480\n",
      "Epoch 77/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.9615 - val_loss: 2.9323\n",
      "Epoch 78/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.9901 - val_loss: 2.9149\n",
      "Epoch 79/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.9562 - val_loss: 2.8868\n",
      "Epoch 80/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.8722 - val_loss: 2.8673\n",
      "Epoch 81/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.8552 - val_loss: 2.8360\n",
      "Epoch 82/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.8208 - val_loss: 2.7734\n",
      "Epoch 83/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8416 - val_loss: 2.7514\n",
      "Epoch 84/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.7712 - val_loss: 2.7384\n",
      "Epoch 85/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.7190 - val_loss: 2.7004\n",
      "Epoch 86/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.7121 - val_loss: 2.6626\n",
      "Epoch 87/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.6932 - val_loss: 2.6263\n",
      "Epoch 88/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.6466 - val_loss: 2.5913\n",
      "Epoch 89/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.6403 - val_loss: 2.5566\n",
      "Epoch 90/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.5782 - val_loss: 2.5339\n",
      "Epoch 91/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.5506 - val_loss: 2.4995\n",
      "Epoch 92/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.5436 - val_loss: 2.4724\n",
      "Epoch 93/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.5128 - val_loss: 2.4526\n",
      "Epoch 94/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5116 - val_loss: 2.4162\n",
      "Epoch 95/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.4454 - val_loss: 2.3954\n",
      "Epoch 96/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.4230 - val_loss: 2.3849\n",
      "Epoch 97/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.3895 - val_loss: 2.3634\n",
      "Epoch 98/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.3642 - val_loss: 2.3278\n",
      "Epoch 99/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.3617 - val_loss: 2.2969\n",
      "Epoch 100/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.2774 - val_loss: 2.2701\n",
      "Epoch 101/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.3001 - val_loss: 2.2514\n",
      "Epoch 102/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.2624 - val_loss: 2.2277\n",
      "Epoch 103/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.2711 - val_loss: 2.1895\n",
      "Epoch 104/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.2100 - val_loss: 2.1703\n",
      "Epoch 105/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.2085 - val_loss: 2.1580\n",
      "Epoch 106/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.1504 - val_loss: 2.1266\n",
      "Epoch 107/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.1562 - val_loss: 2.1037\n",
      "Epoch 108/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.1447 - val_loss: 2.0874\n",
      "Epoch 109/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.1040 - val_loss: 2.0833\n",
      "Epoch 110/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0957 - val_loss: 2.0473\n",
      "Epoch 111/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.1095 - val_loss: 2.0190\n",
      "Epoch 112/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.0592 - val_loss: 2.0121\n",
      "Epoch 113/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9974 - val_loss: 1.9794\n",
      "Epoch 114/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9972 - val_loss: 1.9395\n",
      "Epoch 115/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9870 - val_loss: 1.9281\n",
      "Epoch 116/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9572 - val_loss: 1.9151\n",
      "Epoch 117/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9367 - val_loss: 1.8861\n",
      "Epoch 118/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.8946 - val_loss: 1.8708\n",
      "Epoch 119/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9299 - val_loss: 1.8436\n",
      "Epoch 120/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.8702 - val_loss: 1.8403\n",
      "Epoch 121/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.8147 - val_loss: 1.8263\n",
      "Epoch 122/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.8321 - val_loss: 1.7979\n",
      "Epoch 123/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.7898 - val_loss: 1.7877\n",
      "Epoch 124/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.7904 - val_loss: 1.7487\n",
      "Epoch 125/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7433 - val_loss: 1.7272\n",
      "Epoch 126/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7477 - val_loss: 1.7012\n",
      "Epoch 127/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.7198 - val_loss: 1.6895\n",
      "Epoch 128/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7016 - val_loss: 1.6873\n",
      "Epoch 129/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7266 - val_loss: 1.6707\n",
      "Epoch 130/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6690 - val_loss: 1.6493\n",
      "Epoch 131/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6734 - val_loss: 1.6277\n",
      "Epoch 132/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6646 - val_loss: 1.6242\n",
      "Epoch 133/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6519 - val_loss: 1.6049\n",
      "Epoch 134/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.6215 - val_loss: 1.6004\n",
      "Epoch 135/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5813 - val_loss: 1.5726\n",
      "Epoch 136/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.5393 - val_loss: 1.5514\n",
      "Epoch 137/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5852 - val_loss: 1.5531\n",
      "Epoch 138/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5278 - val_loss: 1.5209\n",
      "Epoch 139/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5181 - val_loss: 1.4902\n",
      "Epoch 140/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.5372 - val_loss: 1.4573\n",
      "Epoch 141/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4774 - val_loss: 1.4415\n",
      "Epoch 142/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4674 - val_loss: 1.4177\n",
      "Epoch 143/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4555 - val_loss: 1.4086\n",
      "Epoch 144/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4379 - val_loss: 1.4103\n",
      "Epoch 145/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3989 - val_loss: 1.4058\n",
      "Epoch 146/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4198 - val_loss: 1.4055\n",
      "Epoch 147/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3762 - val_loss: 1.4112\n",
      "Epoch 148/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4129 - val_loss: 1.3780\n",
      "Epoch 149/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3961 - val_loss: 1.3534\n",
      "Epoch 150/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3728 - val_loss: 1.3436\n",
      "Epoch 151/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3523 - val_loss: 1.3195\n",
      "Epoch 152/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3266 - val_loss: 1.3225\n",
      "Epoch 153/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2957 - val_loss: 1.3224\n",
      "Epoch 154/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3365 - val_loss: 1.3173\n",
      "Epoch 155/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2976 - val_loss: 1.2829\n",
      "Epoch 156/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2769 - val_loss: 1.2607\n",
      "Epoch 157/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2941 - val_loss: 1.2483\n",
      "Epoch 158/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2673 - val_loss: 1.2627\n",
      "Epoch 159/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2776 - val_loss: 1.2468\n",
      "Epoch 160/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2197 - val_loss: 1.2156\n",
      "Epoch 161/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2373 - val_loss: 1.2072\n",
      "Epoch 162/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.1952 - val_loss: 1.1894\n",
      "Epoch 163/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1849 - val_loss: 1.2111\n",
      "Epoch 164/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1786 - val_loss: 1.1979\n",
      "Epoch 165/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1897 - val_loss: 1.1715\n",
      "Epoch 166/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1677 - val_loss: 1.1536\n",
      "Epoch 167/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1680 - val_loss: 1.1350\n",
      "Epoch 168/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1202 - val_loss: 1.1304\n",
      "Epoch 169/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1123 - val_loss: 1.1442\n",
      "Epoch 170/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1092 - val_loss: 1.1339\n",
      "Epoch 171/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.0819 - val_loss: 1.1183\n",
      "Epoch 172/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.0961 - val_loss: 1.0801\n",
      "Epoch 173/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.1021 - val_loss: 1.0706\n",
      "Epoch 174/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.0662 - val_loss: 1.0691\n",
      "Epoch 175/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.0660 - val_loss: 1.0710\n",
      "Epoch 176/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0512 - val_loss: 1.0763\n",
      "Epoch 177/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0191 - val_loss: 1.0700\n",
      "Epoch 178/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.0317 - val_loss: 1.0560\n",
      "Epoch 179/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9908 - val_loss: 1.0289\n",
      "Epoch 180/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9816 - val_loss: 0.9956\n",
      "Epoch 181/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9782 - val_loss: 0.9671\n",
      "Epoch 182/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9831 - val_loss: 0.9863\n",
      "Epoch 183/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9453 - val_loss: 0.9805\n",
      "Epoch 184/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9331 - val_loss: 0.9767\n",
      "Epoch 185/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9394 - val_loss: 0.9704\n",
      "Epoch 186/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9442 - val_loss: 0.9770\n",
      "Epoch 187/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.9232 - val_loss: 0.9690\n",
      "Epoch 188/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9142 - val_loss: 0.9669\n",
      "Epoch 189/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9221 - val_loss: 0.9420\n",
      "Epoch 190/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9148 - val_loss: 0.9285\n",
      "Epoch 191/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8912 - val_loss: 0.9312\n",
      "Epoch 192/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8861 - val_loss: 0.9620\n",
      "Epoch 193/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8822 - val_loss: 0.9316\n",
      "Epoch 194/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8885 - val_loss: 0.8972\n",
      "Epoch 195/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8946 - val_loss: 0.8880\n",
      "Epoch 196/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8342 - val_loss: 0.8771\n",
      "Epoch 197/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8576 - val_loss: 0.8681\n",
      "Epoch 198/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8404 - val_loss: 0.8700\n",
      "Epoch 199/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8214 - val_loss: 0.8436\n",
      "Epoch 200/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8311 - val_loss: 0.8106\n",
      "Epoch 201/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8392 - val_loss: 0.8019\n",
      "Epoch 202/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8253 - val_loss: 0.8009\n",
      "Epoch 203/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8134 - val_loss: 0.8086\n",
      "Epoch 204/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8107 - val_loss: 0.8126\n",
      "Epoch 205/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7930 - val_loss: 0.8124\n",
      "Epoch 206/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7678 - val_loss: 0.7974\n",
      "Epoch 207/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7652 - val_loss: 0.7954\n",
      "Epoch 208/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7663 - val_loss: 0.7796\n",
      "Epoch 209/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7509 - val_loss: 0.7671\n",
      "Epoch 210/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7358 - val_loss: 0.7550\n",
      "Epoch 211/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7402 - val_loss: 0.7499\n",
      "Epoch 212/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7704 - val_loss: 0.7685\n",
      "Epoch 213/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7505 - val_loss: 0.7568\n",
      "Epoch 214/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7502 - val_loss: 0.7343\n",
      "Epoch 215/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6992 - val_loss: 0.7201\n",
      "Epoch 216/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7191 - val_loss: 0.7255\n",
      "Epoch 217/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6983 - val_loss: 0.7295\n",
      "Epoch 218/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7213 - val_loss: 0.7295\n",
      "Epoch 219/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6972 - val_loss: 0.7510\n",
      "Epoch 220/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6843 - val_loss: 0.7219\n",
      "Epoch 221/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7293 - val_loss: 0.7147\n",
      "Epoch 222/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6874 - val_loss: 0.7026\n",
      "Epoch 223/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6810 - val_loss: 0.6910\n",
      "Epoch 224/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6769 - val_loss: 0.6955\n",
      "Epoch 225/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6818 - val_loss: 0.6907\n",
      "Epoch 226/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6745 - val_loss: 0.6915\n",
      "Epoch 227/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6455 - val_loss: 0.6699\n",
      "Epoch 228/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6458 - val_loss: 0.6822\n",
      "Epoch 229/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6698 - val_loss: 0.6816\n",
      "Epoch 230/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6319 - val_loss: 0.6571\n",
      "Epoch 231/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6225 - val_loss: 0.6642\n",
      "Epoch 232/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6140 - val_loss: 0.6459\n",
      "Epoch 233/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6224 - val_loss: 0.6458\n",
      "Epoch 234/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6203 - val_loss: 0.6447\n",
      "Epoch 235/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6290 - val_loss: 0.6596\n",
      "Epoch 236/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5965 - val_loss: 0.6431\n",
      "Epoch 237/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6130 - val_loss: 0.6379\n",
      "Epoch 238/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6211 - val_loss: 0.6193\n",
      "Epoch 239/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5765 - val_loss: 0.6073\n",
      "Epoch 240/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5999 - val_loss: 0.5910\n",
      "Epoch 241/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5840 - val_loss: 0.6030\n",
      "Epoch 242/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5639 - val_loss: 0.6165\n",
      "Epoch 243/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5753 - val_loss: 0.6288\n",
      "Epoch 244/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5505 - val_loss: 0.6277\n",
      "Epoch 245/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5608 - val_loss: 0.6187\n",
      "Epoch 246/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5573 - val_loss: 0.5970\n",
      "Epoch 247/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5692 - val_loss: 0.5762\n",
      "Epoch 248/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5731 - val_loss: 0.5778\n",
      "Epoch 249/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5490 - val_loss: 0.5859\n",
      "Epoch 250/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5461 - val_loss: 0.5713\n",
      "Epoch 251/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5767 - val_loss: 0.5532\n",
      "Epoch 252/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5255 - val_loss: 0.5719\n",
      "Epoch 253/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5529 - val_loss: 0.5627\n",
      "Epoch 254/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5318 - val_loss: 0.5760\n",
      "Epoch 255/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5155 - val_loss: 0.5560\n",
      "Epoch 256/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5216 - val_loss: 0.5764\n",
      "Epoch 257/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5166 - val_loss: 0.5775\n",
      "Epoch 258/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5399 - val_loss: 0.5452\n",
      "Epoch 259/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5191 - val_loss: 0.5463\n",
      "Epoch 260/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5143 - val_loss: 0.5549\n",
      "Epoch 261/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5227 - val_loss: 0.5476\n",
      "Epoch 262/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5142 - val_loss: 0.5602\n",
      "Epoch 263/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5112 - val_loss: 0.5461\n",
      "Epoch 264/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5435 - val_loss: 0.5451\n",
      "Epoch 265/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4938 - val_loss: 0.5128\n",
      "Epoch 266/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5018 - val_loss: 0.5197\n",
      "Epoch 267/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4692 - val_loss: 0.5196\n",
      "Epoch 268/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4846 - val_loss: 0.4806\n",
      "Epoch 269/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4997 - val_loss: 0.4984\n",
      "Epoch 270/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4744 - val_loss: 0.5243\n",
      "Epoch 271/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4997 - val_loss: 0.5224\n",
      "Epoch 272/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5020 - val_loss: 0.5090\n",
      "Epoch 273/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4689 - val_loss: 0.5194\n",
      "Epoch 274/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4666 - val_loss: 0.5193\n",
      "Epoch 275/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4668 - val_loss: 0.5060\n",
      "Epoch 276/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4524 - val_loss: 0.4952\n",
      "Epoch 277/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4654 - val_loss: 0.5120\n",
      "Epoch 278/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4490 - val_loss: 0.5111\n",
      "Epoch 279/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4541 - val_loss: 0.5061\n",
      "Epoch 280/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5194 - val_loss: 0.5057\n",
      "Epoch 281/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4559 - val_loss: 0.4950\n",
      "Epoch 282/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4589 - val_loss: 0.4800\n",
      "Epoch 283/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4138 - val_loss: 0.4684\n",
      "Epoch 284/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4345 - val_loss: 0.4875\n",
      "Epoch 285/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4548 - val_loss: 0.4890\n",
      "Epoch 286/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4560 - val_loss: 0.4889\n",
      "Epoch 287/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4359 - val_loss: 0.4881\n",
      "Epoch 288/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4295 - val_loss: 0.4833\n",
      "Epoch 289/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4275 - val_loss: 0.4584\n",
      "Epoch 290/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4445 - val_loss: 0.4348\n",
      "Epoch 291/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4536 - val_loss: 0.4437\n",
      "Epoch 292/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4642 - val_loss: 0.4555\n",
      "Epoch 293/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4544 - val_loss: 0.4519\n",
      "Epoch 294/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4058 - val_loss: 0.4580\n",
      "Epoch 295/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4238 - val_loss: 0.4663\n",
      "Epoch 296/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3869 - val_loss: 0.4777\n",
      "Epoch 297/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4219 - val_loss: 0.4615\n",
      "Epoch 298/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4426 - val_loss: 0.4620\n",
      "Epoch 299/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4446 - val_loss: 0.4405\n",
      "Epoch 300/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4035 - val_loss: 0.4392\n",
      "Epoch 301/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4165 - val_loss: 0.4274\n",
      "Epoch 302/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4203 - val_loss: 0.4382\n",
      "Epoch 303/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4030 - val_loss: 0.4364\n",
      "Epoch 304/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4054 - val_loss: 0.4369\n",
      "Epoch 305/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3813 - val_loss: 0.4322\n",
      "Epoch 306/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3966 - val_loss: 0.4175\n",
      "Epoch 307/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4122 - val_loss: 0.4250\n",
      "Epoch 308/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4063 - val_loss: 0.4406\n",
      "Epoch 309/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3897 - val_loss: 0.4282\n",
      "Epoch 310/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3931 - val_loss: 0.4325\n",
      "Epoch 311/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4009 - val_loss: 0.4294\n",
      "Epoch 312/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3857 - val_loss: 0.4354\n",
      "Epoch 313/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3716 - val_loss: 0.4407\n",
      "Epoch 314/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4080 - val_loss: 0.4433\n",
      "Epoch 315/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3986 - val_loss: 0.4530\n",
      "Epoch 316/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3751 - val_loss: 0.4392\n",
      "Epoch 317/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3792 - val_loss: 0.4231\n",
      "Epoch 318/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3919 - val_loss: 0.4380\n",
      "Epoch 319/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3933 - val_loss: 0.4614\n",
      "Epoch 320/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3844 - val_loss: 0.4563\n",
      "Epoch 321/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3644 - val_loss: 0.4232\n",
      "Epoch 322/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4166 - val_loss: 0.4019\n",
      "Epoch 323/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3696 - val_loss: 0.4232\n",
      "Epoch 324/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4020 - val_loss: 0.4202\n",
      "Epoch 325/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3668 - val_loss: 0.4180\n",
      "Epoch 326/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3829 - val_loss: 0.4194\n",
      "Epoch 327/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4162 - val_loss: 0.4401\n",
      "Epoch 328/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3528 - val_loss: 0.4508\n",
      "Epoch 329/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3567 - val_loss: 0.4549\n",
      "Epoch 330/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3729 - val_loss: 0.4166\n",
      "Epoch 331/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3575 - val_loss: 0.4095\n",
      "Epoch 332/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3564 - val_loss: 0.4189\n",
      "Epoch 333/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3421 - val_loss: 0.4026\n",
      "Epoch 334/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3898 - val_loss: 0.3984\n",
      "Epoch 335/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3573 - val_loss: 0.4167\n",
      "Epoch 336/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3835 - val_loss: 0.3958\n",
      "Epoch 337/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4200 - val_loss: 0.4209\n",
      "Epoch 338/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3478 - val_loss: 0.4273\n",
      "Epoch 339/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3683 - val_loss: 0.4094\n",
      "Epoch 340/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3364 - val_loss: 0.3992\n",
      "Epoch 341/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3442 - val_loss: 0.3992\n",
      "Epoch 342/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3787 - val_loss: 0.3950\n",
      "Epoch 343/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3565 - val_loss: 0.3891\n",
      "Epoch 344/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3138 - val_loss: 0.3846\n",
      "Epoch 345/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3510 - val_loss: 0.3823\n",
      "Epoch 346/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3474 - val_loss: 0.3875\n",
      "Epoch 347/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3656 - val_loss: 0.3887\n",
      "Epoch 348/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3365 - val_loss: 0.4047\n",
      "Epoch 349/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3538 - val_loss: 0.3975\n",
      "Epoch 350/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3160 - val_loss: 0.3666\n",
      "Epoch 351/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3681 - val_loss: 0.3947\n",
      "Epoch 352/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3222 - val_loss: 0.4336\n",
      "Epoch 353/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3461 - val_loss: 0.4100\n",
      "Epoch 354/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3287 - val_loss: 0.3821\n",
      "Epoch 355/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3401 - val_loss: 0.3802\n",
      "Epoch 356/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3469 - val_loss: 0.3922\n",
      "Epoch 357/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3465 - val_loss: 0.3803\n",
      "Epoch 358/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3086 - val_loss: 0.3804\n",
      "Epoch 359/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3365 - val_loss: 0.3696\n",
      "Epoch 360/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3229 - val_loss: 0.3689\n",
      "Epoch 361/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3484 - val_loss: 0.3499\n",
      "Epoch 362/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3184 - val_loss: 0.3834\n",
      "Epoch 363/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3476 - val_loss: 0.4007\n",
      "Epoch 364/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3560 - val_loss: 0.3769\n",
      "Epoch 365/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3367 - val_loss: 0.3375\n",
      "Epoch 366/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3309 - val_loss: 0.3515\n",
      "Epoch 367/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3458 - val_loss: 0.3679\n",
      "Epoch 368/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3337 - val_loss: 0.3453\n",
      "Epoch 369/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3207 - val_loss: 0.3989\n",
      "Epoch 370/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3415 - val_loss: 0.4045\n",
      "Epoch 371/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3445 - val_loss: 0.3625\n",
      "Epoch 372/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3226 - val_loss: 0.3451\n",
      "Epoch 373/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3172 - val_loss: 0.3632\n",
      "Epoch 374/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3315 - val_loss: 0.3699\n",
      "Epoch 375/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3105 - val_loss: 0.3727\n",
      "Epoch 376/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3208 - val_loss: 0.3508\n",
      "Epoch 377/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3203 - val_loss: 0.3630\n",
      "Epoch 378/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3225 - val_loss: 0.3873\n",
      "Epoch 379/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3274 - val_loss: 0.3768\n",
      "Epoch 380/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3201 - val_loss: 0.3620\n",
      "Epoch 381/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3279 - val_loss: 0.3556\n",
      "Epoch 382/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3147 - val_loss: 0.3607\n",
      "Epoch 383/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3290 - val_loss: 0.3686\n",
      "Epoch 384/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3147 - val_loss: 0.3724\n",
      "Epoch 385/600\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3119 - val_loss: 0.3616\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Training R2 Score: 0.9198506847832365\n",
      "Testing R2 Score: 0.8941338447457491\n",
      "Training Mean Absolute Error: 0.1796867576964778\n",
      "Testing Mean Absolute Error: 0.2374061756819777\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Assuming 'dataset' is already loaded\n",
    "dataset_columns = dataset.columns\n",
    "columns_to_exclude = ['Formula', 'M', 'X', 'T', 'Work_function', 'Heat_of_formation', 'Energy_above_convex_hull', 'Length(x)','Radius_of_orbital_of_support','Electron_affinity_of_termination','Electronegativity_of_support','Ionisation_potential_of_termination','Electron_affinity_of_support']\n",
    "\n",
    "# Preprocess the data\n",
    "predictors = dataset.drop(columns=columns_to_exclude)\n",
    "target = dataset['Heat_of_formation']\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the number of input features\n",
    "n_cols = predictors_norm.shape[1]\n",
    "\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(n_cols,), kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adamax(learning_rate=0.002)\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = regression_model()\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, batch_size=32, validation_split=0.2, epochs=600, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_predictions = model.predict(X_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate R2 score for training and testing sets\n",
    "r2_train = r2_score(y_train, train_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Calculate MAE for training and testing sets\n",
    "mae_train = mean_absolute_error(y_train, train_predictions)\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f'Training R2 Score: {r2_train}')\n",
    "print(f'Testing R2 Score: {r2_test}')\n",
    "print(f'Training Mean Absolute Error: {mae_train}')\n",
    "print(f'Testing Mean Absolute Error: {mae_test}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
